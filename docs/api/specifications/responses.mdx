---
---

# ðŸ’¬ Responses

To an HTTP request, the API returns all results formatted in **JSON**.

Let's have a detailed look at the request example generated on the **[Requests](/api/specifications/requests)** page.

```json
{
    "request_id": "552ba721-7f1c-4c51-8108-eb8b20579721",
    "origin": "create",
    "outputs": [
        {
            "input_text": "Once upon a time, ",
            "completions": [
                {
                    "finish_reason": "length",
                    "output_text": "there was a little girl who loved to read. She read all the time. She read in the car, she read "
                }]
        }]
}
```

## Response format

The response is made of three parts:

-   A **`request_id`**, uniquely identifying the request.
-   An **`origin`**, showing the endpoint used for the request.
-   An **`outputs`** list, containing the model answer to your request, and useful metadata.

## Generation response

The `outputs` list will be structured according to how you have batched your request. It will contain the following structure **per input text** you have submitted.

-   A `finish_reason` entry, explaining why the model stopped processing further tokens (`length` if stopped by `n_tokens` or by reaching the end of the text to process, or `stop_word` if reached one of the `stop_words`).
-   An `output_text` with the generated text by Paradigm.

### Score

The `score` dictionary provides information regarding the log-probabilities of the tokens processed:

-   `logprob` is the overall log-probability of the entire text processed.
-   `normalized_logprob` is the same as above, but normalized for text length (number of tokens).
-   `token_logprobs` is a dictionary including the specific log-probability of each token following the first one.
